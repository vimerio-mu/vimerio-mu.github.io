<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>自证：2019301040100陈泽彬</title>
    <link href="/2020/04/29/%E8%87%AA%E8%AF%81%EF%BC%9A2019301040100%E9%99%88%E6%B3%BD%E5%BD%AC/"/>
    <url>/2020/04/29/%E8%87%AA%E8%AF%81%EF%BC%9A2019301040100%E9%99%88%E6%B3%BD%E5%BD%AC/</url>
    
    <content type="html"><![CDATA[<p>自证：2019301040100陈泽彬</p><p>本文仅供python期末自证使用。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>如何做一个还能看的个人博客(1)</title>
    <link href="/2020/04/20/%E5%A6%82%E4%BD%95%E5%81%9A%E4%B8%80%E4%B8%AA%E8%BF%98%E8%83%BD%E7%9C%8B%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2(1)/"/>
    <url>/2020/04/20/%E5%A6%82%E4%BD%95%E5%81%9A%E4%B8%80%E4%B8%AA%E8%BF%98%E8%83%BD%E7%9C%8B%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2(1)/</url>
    
    <content type="html"><![CDATA[<h2 id="项目起因"><a href="#项目起因" class="headerlink" title="项目起因"></a>项目起因</h2><p>因为想把自己之前做的东西弄成DD-center那样的网页，所以想学一些前端。<del>对我知道这个理由很扯淡</del>，但是我的确这段时间都在学前端，算是把html和css的基础技能都学了一遍吧，一些静态的网页应该是可以做了，<del>可以开始制造电子垃圾了</del>。接下来就要接着学JavaScript和一些框架（比如jQuery和vue.js之类的）了。但在这之前，先来检验一下我前面的学习成果。</p><h2 id="项目设计"><a href="#项目设计" class="headerlink" title="项目设计"></a>项目设计</h2><p>作为一个普通的个人博客，首先要有首页、功能页（个人介绍、标签归总等）、文章详情页三个部分。</p><p>在首页，我希望有一个包含logo和功能页的侧边栏，一个包含文章标题和内容简介的文章列表栏。</p><p>在功能页和详情页，我希望沿用主页的基本设计。对于功能页需要包含一些ui可以跳转到B站等网站中我的个人首页；对于文章页，就正常的沿用我md文件的排版就好。</p><p>在想清楚整个网站的构造之后，让我们进入具体的设计。</p><h4 id="首页设计"><a href="#首页设计" class="headerlink" title="首页设计"></a>首页设计</h4><pre><code class="html">&lt;!DOCTYPE html&gt;&lt;html&gt;    &lt;head&gt;        &lt;meta charset=&quot;utf-8&quot; /&gt;        &lt;title&gt;vimerio&#39;s blog&lt;/title&gt;        &lt;link href=&quot;https://cdn.bootcss.com/normalize/8.0.1/normalize.min.css&quot; rel=&quot;stylesheet&quot;&gt;        &lt;link rel=&quot;stylesheet&quot; href=&quot;./css/main.css&quot;/&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;div class=&quot;side-bar&quot;&gt;        &lt;/div&gt;        &lt;div class=&quot;main&quot;&gt;        &lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;</code></pre><p>首先我们设计两个div，分别是side-bar（侧边栏）和main（主要部分即文章列表）。</p><p>在接着介绍之前，我提一下normalize.css这个css文件，这是一个比较通用的初始化css文件，包括但不限于取消body的margin等等，可以达到很好的初始化效果，提升效率必备（<del>懒狗必备</del>）。</p><p>在设计完第一级的标签之后，我们再设计二级和三级标签。</p><pre><code class="html">&lt;div class=&quot;side-bar&quot;&gt;            &lt;div class=&quot;header&quot;&gt;                &lt;a class=&quot;logo&quot;href=&quot;index.html&quot;&gt;&lt;img src=&quot;./img/678.png&quot;&gt;&lt;/a&gt;                &lt;div class=&quot;intro&quot;&gt;&lt;b&gt;WHU SIM freshman&lt;/b&gt;&lt;br/&gt;                &lt;b&gt; learning field:Data Visualization&lt;/b&gt;&lt;br /&gt;                &lt;b&gt;field:NLP&lt;/b&gt;&lt;/div&gt;            &lt;/div&gt;            &lt;div class=&quot;nav&quot;&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;关于我&lt;/a&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;联系我&lt;/a&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;捐赠&lt;/a&gt;            &lt;/div&gt;            &lt;div class=&quot;tag-list&quot;&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;#python&lt;/a&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;#html&lt;/a&gt;                &lt;a href=&quot;#&quot; class=&quot;item&quot;&gt;#tensorflow&lt;/a&gt;            &lt;/div&gt;        &lt;/div&gt;        &lt;div class=&quot;main&quot;&gt;            &lt;div class=&quot;article-list&quot;&gt;                &lt;div class=&quot;item&quot;&gt;                    &lt;a href=&quot;article.html&quot; class=&quot;title&quot;&gt;lorem&lt;/a&gt;                    &lt;div class=&quot;status&quot;&gt;2020-4-1 |标签：#python&lt;/div&gt;                    &lt;div class=&quot;content&quot;&gt;orem&lt;/div&gt;                &lt;/div&gt;            &lt;/div&gt;        &lt;/div&gt;    &lt;/body&gt;</code></pre><p>我在侧边栏中设计了导航栏（nav）和标签索引（tag-list），并且在其下用a标签整了一些超链接。</p><p>在文章列表中定义了item用来存放单个的文章介绍，并且设置了标题（一个可点击的超链接），状态（如你所见：发布时间，标签）。在这之后，我们就可以开始css的设计了。</p><pre><code class="css">body {    background: #454545;    line-height: 1.7;}/*设置整个页面的底色和基本行高（说实话行高1真的太挤了）*/.side-bar{    float: left;    width: 20%;    position: fixed;    box-shadow: 0 0 5px 2px rgba(0,0,0,0.2);    background-color: #555;}/*我习惯所有div都设置成浮动左，然后侧边栏因为东西比较少就20%的宽度就好；因为我希望侧边栏无论何时都可以显示出来，让用户看比较长的文章的时候也可以看到，于是就设置成了fixed；阴影是模仿卡片的设计，这个网页主要都是卡片式设计。*/.side-bar &gt;* {    padding: 10px 15px;}/*&gt;*是表达所有子元素的写法，这样可以比较方便的同时设计nav和tag-list。在这里设置了一下他们之间的距离，这样看起来不会那么挤。*/.main {    float: right;    width: 80%;    color: #454545;}/*文章列表的浮动、宽度、字体颜色。*/img {    width: 220px;    height: 200px;    border-radius: 50%;}/*logo这里就简单整一下长和宽，并且做成圆形（其实是椭圆）*/a,body{    color:snow ;}/*要设置多个标签的时候用逗号就可以了。*/a {    text-decoration: none;}/*取消下划线*/.nav a,.tag-list a {    display: block;    padding: 5px;    color: #888;    -webkit-transition: color 200ms;    -moz-transition:color 200ms ;    -ms-transition:color 200ms ;    -o-transition:color 200ms ;    transition: color 200ms;}/*这里做了一下字体颜色渐变，这样看起来就没那么突兀。可以用transition+tab快速打出上面的东西*/.nav a:hover,.tag-list a:hover {    color: snow;}/*设置hover（点击时）的颜色。*/.nav a {    font-weight: 700;}/*加粗导航栏字体*/.article-list,.article {    margin-right: 30%;    background: #FFFAFA;    padding: 20px 30px;    box-shadow: 0 0 5px 2px rgba(0,0,0,0.2);}/*同样的给文章列表做卡片效果和底色以及间距的设置。其实说实话底色那么暗卡片挺难看出来的不过前面弄了复制一下也方便*/.title {    font-size:22px;    font-weight: 700;    color: #454545;}/*标题加大加粗改色*/.status {    font-size: 13px;    color: #ccc;}/*状态比较不重要，字体小一点颜色浅一点*/item &gt;* {    margin: 10px 0;}/*同样的用&gt;*定义所有子元素，设置内上下边距*/.article-list.item {    margin-bottom: 25px;}/*底部不要靠太近*/</code></pre><p>效果如下：<a href="http://q8i78k1ua.bkt.clouddn.com/html.jpg" target="_blank" rel="noopener">http://q8i78k1ua.bkt.clouddn.com/html.jpg</a></p><h4 id="文章详情页设计"><a href="#文章详情页设计" class="headerlink" title="文章详情页设计"></a>文章详情页设计</h4><p>在这里推荐一下我现在用markdown编辑器Typora，清爽免费业界清流（<del>结一下</del>）。这个软件有一个md转html的功能。于是我的文章详情页设计还没有开始就结束了…</p><h4 id="导航栏和标签集合"><a href="#导航栏和标签集合" class="headerlink" title="导航栏和标签集合"></a>导航栏和标签集合</h4><p>下次再更，下次一定</p>]]></content>
    
    
    <categories>
      
      <category>web</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-html -css</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>爬取用户关注者并生成其粉丝数和播放数图表</title>
    <link href="/2020/04/07/%E7%88%AC%E5%8F%96%E7%94%A8%E6%88%B7%E5%85%B3%E6%B3%A8%E8%80%85%E5%B9%B6%E7%94%9F%E6%88%90%E5%85%B6%E7%B2%89%E4%B8%9D%E6%95%B0%E5%92%8C%E6%92%AD%E6%94%BE%E6%95%B0%E5%9B%BE%E8%A1%A8/"/>
    <url>/2020/04/07/%E7%88%AC%E5%8F%96%E7%94%A8%E6%88%B7%E5%85%B3%E6%B3%A8%E8%80%85%E5%B9%B6%E7%94%9F%E6%88%90%E5%85%B6%E7%B2%89%E4%B8%9D%E6%95%B0%E5%92%8C%E6%92%AD%E6%94%BE%E6%95%B0%E5%9B%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="项目起因"><a href="#项目起因" class="headerlink" title="项目起因"></a>项目起因</h2><p>其实我从很久之前看了B站相关的数据可视化视频之后就很想尝试一下，但是一直没有动力。正好最近在推vtuber，想着为圈子做些贡献，并且WHUer无学可上，闲着也是闲着。我就自学了一些网络爬虫和数据可视化，虽然不多但是勉强够用，于是就开始了这个项目。</p><h2 id="项目设计"><a href="#项目设计" class="headerlink" title="项目设计"></a>项目设计</h2><p>简单的来说这个项目可以分成三个部分：</p><h4 id="1-根据uid访问用户关注页并获得其关注者的信息（uid）"><a href="#1-根据uid访问用户关注页并获得其关注者的信息（uid）" class="headerlink" title="1.根据uid访问用户关注页并获得其关注者的信息（uid）"></a>1.根据uid访问用户关注页并获得其关注者的信息（uid）</h4><h4 id="2-根据获得的up主uid逐一访问他们的粉丝页并且找到粉丝数和播放量的api"><a href="#2-根据获得的up主uid逐一访问他们的粉丝页并且找到粉丝数和播放量的api" class="headerlink" title="2.根据获得的up主uid逐一访问他们的粉丝页并且找到粉丝数和播放量的api"></a>2.根据获得的up主uid逐一访问他们的粉丝页并且找到粉丝数和播放量的api</h4><h4 id="3-整理数据并制作图表"><a href="#3-整理数据并制作图表" class="headerlink" title="3.整理数据并制作图表"></a>3.整理数据并制作图表</h4><p>下面我来分别介绍项目的三个部分。</p><h2 id="项目实现"><a href="#项目实现" class="headerlink" title="项目实现"></a>项目实现</h2><h4 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h4><p>之前我们已经实现了根据uid获得用户简介和名称。其实原理完全一样，只不过换了网页。但是不同之处在于这里我们要用selenium来模拟浏览器，不然无法看到相关信息。</p><pre><code class="python">from selenium import webdriverfrom bs4 import BeautifulSoupimport reuser=input(&#39;请输入你的uid号：&#39;)uid_href=&#39;&#39;href=[]driver=webdriver.Firefox()driver.get(&#39;https://space.bilibili.com/&#39;+user+&#39;/fans/follow&#39;)html=driver.page_sourcesoup=BeautifulSoup(html,&#39;html.parser&#39;)tags=soup.find_all(attrs={&#39;href&#39;:re.compile(r&quot;//space.bilibili.com/(\d+)/&quot;)})for tag in tags:    if &#39;href&#39; in tag.attrs:        href+=tag.attrs[&#39;href&#39;]    uid_href=&#39;&#39;.join(href)pattern=re.compile(r&#39;\d+&#39;)uids=pattern.findall(uid_href)uids=list(set(uids))</code></pre><h4 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h4><p>这一步我们要获得up主的粉丝数和播放数。如果尝试过你就会发现：这两个数据不是静态的存在于当前页面的源代码中的，而是有一个api将数据传给当前页面的。我们经过观察可以发现API的url分别为：</p><p>‘<a href="https://api.bilibili.com/x/relation/stat?vmid=’+uid" target="_blank" rel="noopener">https://api.bilibili.com/x/relation/stat?vmid=’+uid</a>     (粉丝)</p><p>和</p><p>‘<a href="https://api.bilibili.com/x/space/upstat?mid=&#39;+uid" target="_blank" rel="noopener">https://api.bilibili.com/x/space/upstat?mid=&#39;+uid</a>  （播放）</p><p>接下来我们就获得里面的数据并存为列表就可以了。</p><pre><code class="python">#获得粉丝数followers=[]for i in range(0,len(uids)):    driver.get(&#39;https://api.bilibili.com/x/relation/stat?vmid=&#39;+uids[i])    html=driver.page_source    soup=BeautifulSoup(html,&#39;html.parser&#39;)    data=soup.find(&#39;div&#39;,id=&#39;json&#39;)    data=data.string    data=data.split(&#39;&quot;follower&quot;:&#39;)    follower=data[-1].split(&#39;}}&#39;)    followers+=follower    followers= [i for i in followers if i != &#39;&#39;]#获得播放量views=&#39;&#39;  for i in range(0,len(uids)):    driver.get(&#39;https://api.bilibili.com/x/space/upstat?mid=&#39;+uids[i])    html=driver.page_source    soup=BeautifulSoup(html,&#39;html.parser&#39;)    data=soup.find(&#39;div&#39;,id=&#39;json&#39;)    data=data.string    data=data.split(&#39;{&quot;code&quot;:0,&quot;message&quot;:&quot;0&quot;,&quot;ttl&quot;:1,&quot;data&quot;:{&quot;archive&quot;:{&quot;view&quot;:&#39;)    data=data[1]    data=data.split(&#39;},&quot;article&quot;&#39;)    view=data[0]+&#39;,&#39;    views+=viewviews=views.split(&#39;,&#39;)</code></pre><p>当然为了制作包含名字而不是uid的图表，我们还需要获得up主的名称，这里可以直接用到上一篇文章的代码，这里就不再展示。</p><h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a>第三部分</h4><p>最后我们制作图表就可以了，这里我们使用的是echarts的python版本pyecharts，这个工具可以生成动态图标而且有多个主题可以直接使用，很方便。</p><pre><code class="python">from pyecharts import options as optsfrom pyecharts.charts import Barfrom pyecharts.globals import ThemeTypec = (    Bar(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))    .add_xaxis(name)    .add_yaxis(&quot;粉丝数量&quot;, followers)    .add_yaxis(&quot;播放量&quot;, views)    .set_global_opts(        xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=60)),        title_opts=opts.TitleOpts(title=&quot;hololive&quot;, subtitle=&quot;粉丝数和播放量&quot;),    )    .render(&quot;hololive.html&quot;))</code></pre><p>至此，项目全部完成，效果如下：</p><p><a href="http://q8i78k1ua.bkt.clouddn.com/hololive.html" target="_blank" rel="noopener">hololive20位vtuber的粉丝数和播放数图表</a></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>其实我开始项目的五天前还完全不会任何爬虫，也不会数据可视化。学习并做到这些东西我拢共用了可能不到20个小时。这当然不是我有天分什么的。ted有一个演讲叫’<strong>How To Learn Anything You Want In Just 20 Hours</strong>‘，当然前提是1. Deconstruct the skill 2. Learn Enough to Self-Correct 3. Remove Practice Barriers 4. Practice at least 20 hours。</p><p>我觉得编程十分符合这些：1.编程的代码实现调用了许多第三方库，这些第三方库实际帮助你知道了代码的功能区块，帮助你分解了任务。2.代码的实现与否清晰可见。3.我用手机看视频，用电脑写代码，根本没有其他电子设备让我分神了…可能这就是我这段时间执行力比较高的原因吧。</p><h2 id="代码展示（以爬取hololive为例）"><a href="#代码展示（以爬取hololive为例）" class="headerlink" title="代码展示（以爬取hololive为例）"></a>代码展示（以爬取hololive为例）</h2><pre><code class="python">from selenium import webdriverimport requestsimport seleniumfrom bs4 import BeautifulSoupimport refrom selenium.webdriver.chrome.options import Optionsimport csvimport xlwtdef getuid(uid):    url=&#39;https://space.bilibili.com/&#39;+str(uid)    r=requests.get(url,timeout=30)    return r.textdef cutuid(html):    soup=BeautifulSoup(html,&#39;html.parser&#39;)    ls=soup.find_all(&#39;meta&#39;,content=re.compile(&#39;bilibili&#39;))    #获得简介框（因为简介tag包含&#39;bilibili&#39;)    content=str(ls)    small_content=re.split(&#39;，bilibili是国内知名的视频弹幕网站，这里有最及时的动漫新番，最棒的ACG氛围，最有创意的Up主。大家可以在这里找到许多欢乐。&quot; name=&quot;description&quot;&gt;\n&lt;/meta&gt;&#39;,content)    a=small_content[0]    b=re.split(&#39;meta content=&quot;&#39;,a)    c=b[1]    final_content=re.split(&#39;，&#39;,c,1)    name=final_content[0]    return name#获得hololive众人的uid，286700005是holo哥的uiduid_href=&#39;&#39;href=[]driver=webdriver.Firefox()driver.get(&#39;https://space.bilibili.com/&#39;+user+&#39;/fans/follow&#39;)html=driver.page_sourcesoup=BeautifulSoup(html,&#39;html.parser&#39;)tags=soup.find_all(attrs={&#39;href&#39;:re.compile(r&quot;//space.bilibili.com/(\d+)/&quot;)})for tag in tags:    if &#39;href&#39; in tag.attrs:        href+=tag.attrs[&#39;href&#39;]    uid_href=&#39;&#39;.join(href)pattern=re.compile(r&#39;\d+&#39;)uids=pattern.findall(uid_href)uids=list(set(uids))#获得名称name=[]for i in range(0,len(uids)):    html=getuid(uids[i])    name.append(cutuid(html))#获得粉丝数followers=[]for i in range(0,len(uids)):    driver.get(&#39;https://api.bilibili.com/x/relation/stat?vmid=&#39;+uids[i])    html=driver.page_source    soup=BeautifulSoup(html,&#39;html.parser&#39;)    data=soup.find(&#39;div&#39;,id=&#39;json&#39;)    data=data.string    data=data.split(&#39;&quot;follower&quot;:&#39;)    follower=data[-1].split(&#39;}}&#39;)    followers+=follower    followers= [i for i in followers if i != &#39;&#39;]#获得播放量views=&#39;&#39;  for i in range(0,len(uids)):    driver.get(&#39;https://api.bilibili.com/x/space/upstat?mid=&#39;+uids[i])    html=driver.page_source    soup=BeautifulSoup(html,&#39;html.parser&#39;)    data=soup.find(&#39;div&#39;,id=&#39;json&#39;)    data=data.string    data=data.split(&#39;{&quot;code&quot;:0,&quot;message&quot;:&quot;0&quot;,&quot;ttl&quot;:1,&quot;data&quot;:{&quot;archive&quot;:{&quot;view&quot;:&#39;)    data=data[1]    data=data.split(&#39;},&quot;article&quot;&#39;)    view=data[0]+&#39;,&#39;    views+=viewviews=views.split(&#39;,&#39;)print(uids)print(name)print(followers)print(views)#作图from pyecharts import options as optsfrom pyecharts.charts import Barfrom pyecharts.globals import ThemeTypec = (    Bar(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))    .add_xaxis(name)    .add_yaxis(&quot;粉丝数量&quot;, followers)    .add_yaxis(&quot;播放量&quot;, views)    .set_global_opts(        xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=60)),        title_opts=opts.TitleOpts(title=&quot;hololive&quot;, subtitle=&quot;粉丝数和播放量&quot;),    )    .render(&quot;hololive.html&quot;))</code></pre><h4 id><a href="#" class="headerlink" title></a></h4>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-爬虫 -B站 -数据可视化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>爬取B站弹幕信息制作词云</title>
    <link href="/2020/04/03/%E7%88%AC%E5%8F%96B%E7%AB%99%E5%BC%B9%E5%B9%95%E4%BF%A1%E6%81%AF%E5%88%B6%E4%BD%9C%E8%AF%8D%E4%BA%91/"/>
    <url>/2020/04/03/%E7%88%AC%E5%8F%96B%E7%AB%99%E5%BC%B9%E5%B9%95%E4%BF%A1%E6%81%AF%E5%88%B6%E4%BD%9C%E8%AF%8D%E4%BA%91/</url>
    
    <content type="html"><![CDATA[<h3 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h3><h5 id="获得弹幕"><a href="#获得弹幕" class="headerlink" title="获得弹幕"></a>获得弹幕</h5><pre><code class="python">import reimport requestsimport bs4from bs4 import BeautifulSoup#获取弹幕def getDM(oid):    r=requests.get(&#39;https://api.bilibili.com/x/v1/dm/list.so?oid=&#39;+oid)    r.encoding=r.apparent_encoding    return r.text#截取有效弹幕部分def cutDM(html):    soup=BeautifulSoup(html,&#39;html.parser&#39;)    ls=soup.find_all(&#39;d&#39;)    danmu=&#39;&#39;    for i in ls:        content=&#39;&#39;.join(i.string)        danmu+=content    #content=str(ls)    #pattern=re.compile(&#39;&lt;d.*?&gt;(.*?)&lt;/d&gt;&#39;)    #danmu=pattern.findall(content)    return danmudef saveDM(danmu):    f=open(&#39;danmu.txt&#39;,&#39;wt&#39;,encoding=&#39;utf-8&#39;)    danmu=str(danmu)    f.write(danmu)def main():    oid=input(&#39;请输入oid:&#39;)    html=getDM(oid)    danmu=cutDM(html)    saveDM(danmu)main()</code></pre><h5 id="制作词云"><a href="#制作词云" class="headerlink" title="制作词云"></a>制作词云</h5><pre><code class="python">import wordcloudfrom matplotlib.pyplot import imreadimport jiebatxt=open(&#39;danmu.txt&#39;,&#39;r&#39;,encoding=&#39;UTF-8&#39;).read()ls=jieba.lcut(txt)content=&#39; &#39;.join(ls)print(content)mk=imread(&#39;C:/Users/ASUS/Pictures/sui1.jpg&#39;)w=wordcloud.WordCloud(font_path=&#39;msyh.ttc&#39;,width=700,height=1000,background_color=&#39;white&#39;,mask=mk)w.generate(content)w.to_file(&#39;sui.png&#39;)</code></pre><h3 id="任务分析"><a href="#任务分析" class="headerlink" title="任务分析"></a>任务分析</h3><p>首先爬取弹幕制作词云任务很明显的分为两个部分：数据爬取和数据分析。数据爬取时我们需要爬取B站的弹幕信息，但是由于弹幕信息并非直接显示在当前视频页面的html下，所以我们必须使用’检查‘来寻找存放弹幕信息的xml。爬取好了的弹幕我们需要进行处理，裁取出有效的部分，再进行词频的统计以及词云的制作。</p><h5 id="弹幕爬取"><a href="#弹幕爬取" class="headerlink" title="弹幕爬取"></a>弹幕爬取</h5><p>在这个程序中，我们首先使用requests库进行网页爬取，但是由于这个例子比较简单，你也可以使用python自带的网络爬虫工具来完成。</p><p>由于B站弹幕页面header的编码格式并不是支持中文的utf-8，所以我们要更改页面编码格式，你可以把编码格式手动改为utf-8，也可以将编码格式改成页面实际编码格式。</p><p>然后我们可以使用r.text获得页面的所有信息。如果你还想再看一次的话，使用print()函数。</p><pre><code class="python">import requestsr=requests.get(&#39;https://api.bilibili.com/x/v1/dm/list.so?oid=&#39;+oid)r.encoding=r.apparent_encodinghtml=r.textprint(html)</code></pre><h5 id="弹幕裁取"><a href="#弹幕裁取" class="headerlink" title="弹幕裁取"></a>弹幕裁取</h5><p>在爬取好页面信息html后，我们使用BeautifulSoup库进行html页面的解析。将html解析成beautifulsoup类。</p><pre><code class="python">soup=BeautifulSoup(html,&#39;html.parser&#39;)</code></pre><p>在任务执行前对页面的分析或者刚刚我们用print()函数所看到的页面信息中，我们可以发现：弹幕内容被存放到了d标签中的字符串中。所以我们可以通过soup.find_all()来获得所有d标签。然后我们使用一个for循环获得单个标签，再通过tag.string 来获得b标签的字符串内容，即我们所需要的弹幕内容。</p><pre><code>ls=soup.find_all(&#39;d&#39;)danmu=&#39;&#39;for i in ls:   content=&#39;&#39;.join(i.string)   danmu+=content</code></pre><p>当然，你要是不嫌烦的话，可以按照网上许多教程所说的那样获得d标签的所有内容，然后再用正则表达式截取出弹幕内容。当然这样就比较麻烦，显然辜负了B站程序员们清楚的代码风格。<del>但是这锻炼了你正则表达式的熟练度。</del></p><pre><code class="python">content=str(ls)pattern=re.compile(&#39;&lt;d.*?&gt;(.*?)&lt;/d&gt;&#39;)danmu=pattern.findall(content)</code></pre><h5 id="词云制作"><a href="#词云制作" class="headerlink" title="词云制作"></a>词云制作</h5><p>最后我们需要制作词云，由于B站弹幕绝大部分是中文，我们可以使用jieba库对弹幕内容进行分词。如果你想要更好的分词结果：比如支持日文和其他语言，你可能需要其他的工具支持。</p><pre><code class="python">import jiebatxt=open(&#39;danmu.txt&#39;,&#39;r&#39;,encoding=&#39;UTF-8&#39;).read()ls=jieba.lcut(txt)content=&#39; &#39;.join(ls)     #空格分隔词语，方便下一步词云的制作print(content)            #检查</code></pre><p>分词后我们就可以使用Wordcloud库进行词云制作了。如果你想要制作一个特别的词云，你可能需要mask的支持。mask可以理解为蒙版，可以使你的词云呈现特定的形状。你需要使用matplotlib.pyplot库来读取你的图片mask。最后将词云文件保存，任务完成。最后我们得到的结果如下：<a href="http://q8i78k1ua.bkt.clouddn.com/sui.png" target="_blank" rel="noopener">词云效果</a></p><pre><code class="python">import wordcloudfrom matplotlib.pyplot import imreadmk=imread(&#39;C:/Users/ASUS/Pictures/sui1.jpg&#39;)w=wordcloud.WordCloud(font_path=&#39;msyh.ttc&#39;,width=700,height=1000,background_color=&#39;white&#39;,mask=mk)w.generate(content)w.to_file(&#39;sui.png&#39;)</code></pre><p>当然，这个任务还有很多可以优化的地方，比如我们可以直接通过BV号获得oid，而不用去寻找视频的oid；我们可以批量获得一个用户的所有视频的数据，制作数据量更大的词云；我们还可以支持多语言的分词，设置词表来达到更好的分词效果。有空我会继续更新。</p><h3 id="优化1：直接通过BV号获得oid（4-4完成）"><a href="#优化1：直接通过BV号获得oid（4-4完成）" class="headerlink" title="优化1：直接通过BV号获得oid（4.4完成）"></a>优化1：直接通过BV号获得oid（4.4完成）</h3><p>如果直接使用requests库的get去爬取页面的话，我们无法看到视频的oid，所以为了模拟人真实的行为来获得我们所需要的数据，我们需要使用selenium库来模仿一个浏览器进行操作。</p><pre><code class="python">from selenium import webdriverdriver=webdriver.Firefox()driver.get(&#39;https://www.bilibili.com/video/&#39;+bv)</code></pre><p>上面我们模拟了火狐浏览器对url进行访问，这时候我们就可以做到所见即所得了。</p><p>对视频url进行爬取之后我们需要对我们获得的无比复杂的视频源页面的oid进行分析，具体过程如下：</p><pre><code class="python">soup=BeautifulSoup(html,&#39;html.parser&#39;)content=soup.find(string=re.compile(r&#39;.bilivideo.com/upgcxcode&#39;))a=&#39;&#39;a+=contentb=a.split(&#39;base_url&#39;)c=&#39;&#39;c+=b[0]d=c.split(&#39;upgcxcode&#39;)e=&#39;&#39;e+=d[1]content=e.split(&#39;/&#39;,4)oid=content[3]</code></pre><p>这里的思路就是用split不断进行切割，最后得到视频的oid。</p><p>接下来的步骤就和之前一样了。</p><h3 id="优化2：批量获得一个用户的所有视频的数据（4-4完成）"><a href="#优化2：批量获得一个用户的所有视频的数据（4-4完成）" class="headerlink" title="优化2：批量获得一个用户的所有视频的数据（4.4完成）"></a>优化2：批量获得一个用户的所有视频的数据（4.4完成）</h3><p>想了想很简单，其实优化1和2对之前的我的关键难点就是无法模拟人的操作，在掌握了selenium之后这些难题就迎刃而解了。</p><p>这个优化可以这样实现：</p><p>1.获取用户的uid并访问其空间</p><p>2.获取空间中的视频bv号</p><p>3.沿用优化1的方法</p><h3 id="优化3：更好的分词效果"><a href="#优化3：更好的分词效果" class="headerlink" title="优化3：更好的分词效果"></a>优化3：更好的分词效果</h3>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-爬虫
-B站</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的博客说明</title>
    <link href="/2020/04/02/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87blog/"/>
    <url>/2020/04/02/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87blog/</url>
    
    <content type="html"><![CDATA[<h3 id="为啥整了这个玩意"><a href="#为啥整了这个玩意" class="headerlink" title="为啥整了这个玩意"></a>为啥整了这个玩意</h3><p>其实说实在的也没有什么原因，在翻关注的up主‘codesheep’的视频时看到了有教怎么用hexo搭建个人博客的。<del>正好我今天也不怎么想学习</del>，于是就整了一个…虽然但是，整这个的过程真的挺麻烦的，如果有感兴趣的也可以去看一下（因为他是macOS系统，所以Windows上有很多操作不一样，建议开着弹幕看）。</p><h3 id="要用这玩意干啥"><a href="#要用这玩意干啥" class="headerlink" title="要用这玩意干啥"></a>要用这玩意干啥</h3><p>毕竟是心血来潮弄的博客，所以具体要做什么也没有很清楚的想法。但是出于不想要和其他平台的功能重复，所以这边基本上是不会发一些随笔之类的，主要会围绕编程、视频剪辑相关的比较偏所谓‘极客’一点的东西。</p><p>目前的想法是每月计划，每周学习总结之类的都会发上来。毕竟要真写纯技术类的我可能要先自我禁言五六个月…</p><h3 id="其他相关"><a href="#其他相关" class="headerlink" title="其他相关"></a>其他相关</h3><p>暂时也没有什么要说的了，どぞ、よろしくお願いします！</p>]]></content>
    
    
    <categories>
      
      <category>-计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-生活
-计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/04/02/hello-world/"/>
    <url>/2020/04/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
